{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0605c0",
   "metadata": {},
   "source": [
    "#1: 1.Simple linear regression uses a straight line (i.e. linear) to model the relationship between an independent variable and a dependent variable.\n",
    "Multiple linear regression expands on this by using two or more independent variables to predict the dependent variable. Such models can take more factors into account at the same time, and therefore usually produce more accurate predictions.\n",
    "    2.Continuous variable: Represents a variable that can take on a wide range of values, such as age or height, allowing for more detailed predictions.\n",
    "Indicator variable: Also known as a binary or dummy variable, represents categorical data (e.g., 0 or 1 for two categories). It captures group-based effects by moving the regression line up or down according to category.\n",
    "    3.When a single indicator variable is introduced in a multiple linear regression model with continuous variables, it allows the model to capture different intercepts for each category while maintaining a single slope.\n",
    "    4.Introducing an interaction term between continuous and indicator variables allows the model to vary the slope and intercept between categories. This captures not only different starting points (intercepts) for different groups, but also different rates of change (slopes) for different groups.\n",
    "    5.If only indicator variables derived from non-binary categorical variables (e.g. colour: red, blue, green) are used, the model captures the effect of each category on the dependent variable independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20249109",
   "metadata": {},
   "source": [
    "#4:Many of the coefficients are greater than 10 along with strong or very strong evidence against the null hypothesis of ‘no effect’’ refers to the strength and statistical significance of the individual predictors. A large coefficient (e.g., 10 or more) with strong evidence against the null hypothesis indicates that the predictor has a significant effect on the dependent variable, all else being equal.\n",
    "This significance is based on hypothesis testing; if the predictor is consistently correlated with changes in the outcome variable, it remains statistically significant even if its contribution to variance is small.\n",
    "    In short, the model fit (R^2) reflects how well all predictors together explain the outcome, whereas the statistical significance of coefficients reflects whether each predictor individually is likely to have a real, non-zero effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef0969",
   "metadata": {},
   "source": [
    "#5:\n",
    "    \n",
    "    1.This cell prepares the data for modelling. First, it sets a 50-50 split size based on half the total number of rows in the dataset pokeaman.\n",
    "    For reproducibility, a random seed is set using np.random.seed(130), which ensures that the same random split occurs every time the code is run.\n",
    "    \n",
    "    2.The unit defines a simple linear regression model (Model 3) with HP (Hit Points) as the dependent variable and Attack and Defence as the independent variables.\n",
    "    \n",
    "    3.Here, the fitted model 3 is used to make predictions (yhat_model3) on the test dataset (pokeaman_test).\n",
    "    \n",
    "    4.This unit defines a more complex model (Model 4) that includes multiple interaction terms.\n",
    "    \n",
    "    5.This unit assesses the predictive performance of Model 4 on test data, similar to the assessment of Model 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32346701",
   "metadata": {},
   "source": [
    "#6: In Model 4, the interaction terms generated many new predictor variables, resulting in a highly multicollinear design matrix. The multicollinearity makes the model very sensitive to small changes in the data, leading to poor out-of-sample generalisation. Centring and scaling reduces multicollinearity as shown in Model 3, but in Model 4 the interactions are so complex that even centring and scaling do not stabilise the model (the condition number is very high). As a result, Model 4 overfits the training data and struggles to generalise well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b697f",
   "metadata": {},
   "source": [
    "#7:Each model builds on the previous model by gradually increasing its complexity, striking a balance between expanding predictive power and managing multicollinearity. Starting with the base model (model3_fit), interactions are first added in model4_fit and then scaled back in model5_linear_form, focusing on categorical effects. Finally, model7_linear_form adds selected interactions discreetly, and model7_linear_form_CS applies centring and scaling to reduce multicollinearity and improve the stability of out-of-sample predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc65e00",
   "metadata": {},
   "source": [
    "#9:The code demonstrates how models trained on a particular subset (or generation) of data can be difficult to generalise when applied to other unseen subsets of data. By comparing in-sample and out-of-sample R^2 across generations, the code illustrates the potential limitations of training models on narrow datasets and highlights the importance of diverse training data to enhance generalisation to new unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2093a",
   "metadata": {},
   "source": [
    "chat history:https://chatgpt.com/share/67364786-42b4-8005-8ac1-16258eb0a7cb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
